{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb744a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.228 ðŸš€ Python-3.11.5 torch-2.1.1 CPU (Intel Core(TM) i5-7300U 2.60GHz)\n",
      "Setup complete âœ… (4 CPUs, 7.9 GB RAM, 135.8/237.8 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    " \n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "from ultralytics import YOLO\n",
    "\n",
    "#!pip install mysql-connector-python\n",
    "import mysql.connector\n",
    "#!pip install sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#BID = \"B456\"\n",
    "#FID = \"123\"\n",
    "\n",
    "BID = \"\"\n",
    "FID = \"\"\n",
    "BATABASE_API_BASE_URL = \"http://ec2-52-65-94-246.ap-southeast-2.compute.amazonaws.com:5000\"\n",
    "TOBE_PREDICT_IMAGE_PATH = f\"./images/need_to_predict_{BID}_{FID}.png\"\n",
    "RESULT_IMG_PATH =  f\"./runs/detect/predict/need_to_predict_{BID}_{FID}.png\"\n",
    "MODEL = YOLO(\"./model//best.pt\")\n",
    "CONFIDENCE_LEVEL = 0.5\n",
    "MEAN_FRAMES_PER_HIVE = 8\n",
    "\n",
    "ACCESS_KEY_ID = \"\"\n",
    "SECRET_ACCESS_KEY_ID = \"\"\n",
    "BUCKET_NAME = \"broodbox-thermal-images\"\n",
    "\n",
    "MYSQL_CREDENTIALS_MAIN = {\"host\":\"127.0.0.1\", \"user\":\"dilshan\", \"password\":\"1234\", \"database\":\"broodbox\", \"port\":3306}\n",
    "MYSQL_CREDENTIALS = {\"host\":\"127.0.0.1\", \"user\":\"dilshan\", \"password\":\"1234\", \"database\":\"broodbox_results\", \"port\":3306}\n",
    "MYSQL_RESULRS_TABLE_PREFIX = \"ml_results\"\n",
    "HIVE_DETAILS_TABLE_PREFIX =\"hive_details\"\n",
    "ANALYTICS_DATA_TABLE_PREFIX = \"analytics\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5695e9d8",
   "metadata": {},
   "source": [
    "## Database functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "698bbadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mysql_table(dataset, table_name, credentials=MYSQL_CREDENTIALS):\n",
    "    \n",
    "    \"\"\" This function creates a table in mysql database using pandas dataframe\"\"\"\n",
    "    \n",
    "    engine = create_engine(f'mysql+mysqlconnector://{credentials[\"user\"]}:{credentials[\"password\"]}@{credentials[\"host\"]}:{credentials[\"port\"]}/{credentials[\"database\"]}', connect_args={\"connect_timeout\": 28800})\n",
    "    \n",
    "    if \"classes\" in dataset.columns:\n",
    "        # Serialize lists into JSON strings\n",
    "        dataset[\"image_name\"] = dataset[\"image_name\"].apply(json.dumps)\n",
    "        dataset[\"image_name\"] = dataset[\"image_name\"].str.replace('\"', '')\n",
    "        dataset[\"classes\"] = dataset[\"classes\"].apply(json.dumps)\n",
    "        dataset[\"confidence\"] = dataset[\"confidence\"].apply(json.dumps)\n",
    "\n",
    "    dataset.to_sql(table_name, con=engine, if_exists='replace', index=False)\n",
    "    engine.dispose()\n",
    "\n",
    "    \n",
    "def delete_data(table_name,area_code,location_code,credentials=MYSQL_CREDENTIALS):  \n",
    "    \"\"\"This function will delete the raws where the area_code and location_code matches.\"\"\"\n",
    "    \n",
    "    # Connect to the MySQL server\n",
    "    connection = mysql.connector.connect(\n",
    "        host=credentials[\"host\"],\n",
    "        user=credentials[\"user\"],\n",
    "        password=credentials[\"password\"],\n",
    "        database=credentials[\"database\"]\n",
    "    )\n",
    "    \n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    insert_sql = f\"\"\"\n",
    "    DELETE FROM {table_name} WHERE area_code='{area_code}' AND location_code='{location_code}'\n",
    "    \"\"\"\n",
    "    cursor.execute(insert_sql)\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "\n",
    "def insert_multiple_raws(table_name, data, credentials=MYSQL_CREDENTIALS):\n",
    "    \"\"\" This function will inset multiple raws of data points to the given table.\n",
    "    the insert data shoulb be a dictionary\"\"\"\n",
    "    \n",
    "    # Connect to the MySQL server\n",
    "    connection = mysql.connector.connect(\n",
    "        host=credentials[\"host\"],\n",
    "        user=credentials[\"user\"],\n",
    "        password=credentials[\"password\"],\n",
    "        database=credentials[\"database\"]\n",
    "    )\n",
    "    \n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    # Prepare the INSERT query\n",
    "    insert_query = f\"\"\"\n",
    "    INSERT INTO {table_name} (area_code, location_code, image_name, classes, confidence, active_frame_count)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    # dictionary as a list of tuples of data points\n",
    "    insert_values =[(area_code, location_code, image_name, json.dumps(classes), json.dumps(confidence), active_frame_count) \n",
    "                    for area_code, location_code, image_name, classes, confidence, active_frame_count \n",
    "                    in list(zip(data['area_code'], data['location_code'], data['image_name'], data['classes'], data['confidence'], data['active_frame_count']))]\n",
    "\n",
    "    # Execute the INSERT query with executemany\n",
    "    cursor.executemany(insert_query, insert_values) \n",
    "    # Commit the transaction\n",
    "    connection.commit()  \n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "\n",
    "def select_data_using_query(select_query,credentials=MYSQL_CREDENTIALS):\n",
    "    \"\"\" This function selects data according to given query.\n",
    "    Then returs list of tuples like [(area_code1, location_code1, total_active_frame_count1),.........()]\"\"\"\n",
    "\n",
    "    # Connect to the MySQL server\n",
    "    connection = mysql.connector.connect(\n",
    "        host=credentials[\"host\"],\n",
    "        user=credentials[\"user\"],\n",
    "        password=credentials[\"password\"],\n",
    "        database=credentials[\"database\"]\n",
    "    )  \n",
    "    cursor = connection.cursor()   \n",
    "    cursor.execute(select_query)\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def update_active_frame_counts(data_table_name, update_table_name, data_credentials=MYSQL_CREDENTIALS, update_credentials=MYSQL_CREDENTIALS_MAIN):\n",
    "    \"\"\" This function first gets the results of select_active_frame_counts function and then update the hive_details_BID_FID table's total_active_frames\n",
    "    column in the broodbox database\"\"\"\n",
    "\n",
    "    select_sql = f\"\"\"\n",
    "                    SELECT area_code,location_code,SUM(active_frame_count) AS total_active_frames FROM {data_table_name} GROUP BY area_code,location_code;\n",
    "                \"\"\"\n",
    "\n",
    "    # gets the results of select_active_frame_counts \n",
    "    update_values = select_data_using_query(select_sql, data_credentials) \n",
    "    # Connect to the MySQL server\n",
    "    connection = mysql.connector.connect(\n",
    "        host=update_credentials[\"host\"],\n",
    "        user=update_credentials[\"user\"],\n",
    "        password=update_credentials[\"password\"],\n",
    "        database=update_credentials[\"database\"]\n",
    "    )  \n",
    "    cursor = connection.cursor()\n",
    "    # updation\n",
    "    for update_data in update_values:\n",
    "        active_frame_count =  update_data[2]\n",
    "        area_code = update_data[0]\n",
    "        location_code = update_data[1]\n",
    "        # Use parameterized query to update values\n",
    "        update_sql = \"\"\"\n",
    "                UPDATE {}\n",
    "                SET total_active_frames = %s\n",
    "                WHERE area_code = %s AND location_code = %s\n",
    "            \"\"\".format(update_table_name)\n",
    "\n",
    "        cursor.execute(update_sql, (active_frame_count, area_code, location_code))\n",
    "\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522f7c4f",
   "metadata": {},
   "source": [
    "### Analytics functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4cb2498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_ranking(number_list):\n",
    "    \"\"\" This function returns list of ranks of maximun to minimun according to given numeric list.\n",
    "    ex: input = [15,11,16,17,18] then the output is = [4,5,3,2,1]. this is used to get the global ranking list of each hive location according to location richness\n",
    "    of each hive location.\"\"\"\n",
    "    # sorte the given list in decending order\n",
    "    sorted_number_list = sorted(set(number_list), reverse=True)\n",
    "    rank_list = []\n",
    "\n",
    "    for num in number_list:\n",
    "        rank = sorted_number_list.index(num)+1\n",
    "        rank_list.append(rank)\n",
    "\n",
    "    return rank_list\n",
    "\n",
    "def get_local_ranking(tuple_list,location_richness):\n",
    "    \"\"\" This function get list of tuples that contained area code,location code,total_beehives,total_active_frames as tuple_list parameter.\n",
    "    And location richness list as another parameter. then this will returns local ranking list according to the location richness of each hive location.\"\"\"\n",
    "    # This dict contains each area code as key and its richness values as a list \n",
    "    area_richness_dic = dict()\n",
    "    for i in range(len(tuple_list)):\n",
    "        area = tuple_list[i][0]\n",
    "        richness = location_richness[i]\n",
    "\n",
    "        if area not in area_richness_dic:\n",
    "            area_richness_dic[area] = []  \n",
    "        area_richness_dic[area].append(richness)\n",
    "    # This dict contains each area code as key and its local ranking of richness according to the area code\n",
    "    local_ranking_dict = dict()    \n",
    "    for key,value in area_richness_dic.items():\n",
    "        if key not in local_ranking_dict:\n",
    "            local_ranking_dict[key] = get_global_ranking(value)\n",
    "    # this list contains local_ranking_dict dictionary's values as one extended list \n",
    "    local_ranking_list = []\n",
    "    for key,value in local_ranking_dict.items():\n",
    "        local_ranking_list.extend(value)\n",
    "    \n",
    "    return local_ranking_list\n",
    "\n",
    "def create_analytics_table(BID,FID,credentials=MYSQL_CREDENTIALS):\n",
    "    \"\"\" This function cretes analytics table by using hive details table and ml results tables.\"\"\"\n",
    "\n",
    "    data_table_name = f\"hive_details_{BID}_{FID}\"\n",
    "    select_sql = f\"\"\"\n",
    "                        SELECT area_code,location_code,total_beehives,total_active_frames FROM {data_table_name};\n",
    "                    \"\"\"\n",
    "    # get hive detail as list of tuples\n",
    "    results = select_data_using_query(select_sql, credentials=MYSQL_CREDENTIALS_MAIN)\n",
    "    # extract column data for the analytics table\n",
    "    location_richness =  [round((res_tuple[3]/(res_tuple[2]*MEAN_FRAMES_PER_HIVE))*100,2) for res_tuple in results]\n",
    "    global_ranking = get_global_ranking(location_richness)\n",
    "    local_ranking = get_local_ranking(results,location_richness)\n",
    "    area_codes = [result[0] for result in results]\n",
    "    location_codes = [result[1] for result in results]\n",
    "    total_beehives = [result[2] for result in results]\n",
    "    total_active_frames = [result[3] for result in results]\n",
    "    # creates dataframe \n",
    "    data_dic = {\"area_code\":area_codes, \"location_code\":location_codes, \"total_beehives\":total_beehives, \"total_active_frames\":total_active_frames, \"location_richness\":location_richness, \"global_ranking\":global_ranking,\"local_ranking\":local_ranking}\n",
    "    dataframe  = pd.DataFrame(data_dic)\n",
    "    # creates analytics table\n",
    "    analytics_table_name = f\"{ANALYTICS_DATA_TABLE_PREFIX}_{BID}_{FID}\"\n",
    "    create_mysql_table(dataframe, analytics_table_name,credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9a8cb9",
   "metadata": {},
   "source": [
    "## ML prediction functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb7aabc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_area_location_codes(BID,FID):\n",
    "    \n",
    "    \"\"\" This function returns a dict that contained each area code as keys and location codes of each area code as values.\n",
    "     the output format is {\"area_code1\":[list of location codes of that area1],.....,} \"\"\"\n",
    "    \n",
    "    url = f\"{BATABASE_API_BASE_URL}/hive/area-location-codes/{BID}/{FID}\"\n",
    "    response = requests.get(url)\n",
    "    # this contaied all area and loation codes separately as lists of a given farm\n",
    "    data = response.json()\n",
    "    area_codes = data[\"area_codes\"]\n",
    "\n",
    "    url_all = f\"{BATABASE_API_BASE_URL}/hive/{BID}/{FID}\"\n",
    "    response_all = requests.get(url_all)\n",
    "    # this contained all the hive details of given farm\n",
    "    data_all = response_all.json()\n",
    "\n",
    "    # this dict is the requried output format. it should contained as {\"area_code\":[list of location codes of that area]}\n",
    "    codes_dict = dict()\n",
    "    for area_code in area_codes:\n",
    "        codes_dict[area_code] = []\n",
    "\n",
    "        for location in data_all[\"hive_details\"]:\n",
    "\n",
    "            if location[\"area_code\"]==area_code:\n",
    "                codes_dict[area_code].append(location[\"location_code\"])\n",
    "                \n",
    "    return codes_dict \n",
    "\n",
    "\n",
    "def orientaion_correction(img):\n",
    "    \n",
    "    \"\"\"This function checks the given image rotated (480*640 ---> 640*480) or not. \n",
    "    if rotated then it rotate again to oraginal format (480*640 or 640*480 ) and returns the image.\n",
    "    if not rotated then it returns the original image.\"\"\"\n",
    "    \n",
    "    # getexif attribute is a method used to retrieve Exif (Exchangeable image file format) metadata from the image.\n",
    "    if hasattr(img, '_getexif') and img._getexif():\n",
    "        exif_data = img._getexif()\n",
    "        # 274 represents the Exif tag for orientation\n",
    "        orientation = exif_data.get(274) \n",
    "\n",
    "        # If orientation is 6, rotate clockwise by 90 degrees\n",
    "        if orientation == 6:  \n",
    "            img = img.transpose(method=Image.ROTATE_270)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def create_results_folder_tree(BID,FID,codes_dict):\n",
    "    \n",
    "    \"\"\" This function will creates object detection results saving folder structure like data folder\"\"\"\n",
    "    \n",
    "    # get connection with s3 and create resluts folder\n",
    "    s3_client = boto3.client('s3', aws_access_key_id= ACCESS_KEY_ID,\n",
    "                    aws_secret_access_key=SECRET_ACCESS_KEY_ID)\n",
    "    s3_client.put_object(Bucket=BUCKET_NAME, Key=f\"images_{BID}/results_{FID}/\")\n",
    "\n",
    "    for area_code,location_list in codes_dict.items():\n",
    "        s3_client.put_object(Bucket=BUCKET_NAME, Key=f\"images_{BID}/results_{FID}/{area_code}/\")\n",
    "\n",
    "        for location_code in location_list:\n",
    "            s3_client.put_object(Bucket=BUCKET_NAME, Key=f\"images_{BID}/results_{FID}/{area_code}/{location_code}/\")\n",
    "            \n",
    "def is_folder_exist(folder,bucket):\n",
    "    \n",
    "    \"\"\" If given folder exist bucket then this will returns true. other wise false.\"\"\"\n",
    "    \n",
    "    bucket_list = list(bucket.objects.all().filter(Prefix=f\"{folder}/\"))\n",
    "    if len(bucket_list) >=1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def upload_image(RESULT_IMG_PATH,save_path,bucket_name):\n",
    "    \n",
    "    \"\"\" This function save image at given save_path at s3 bucket\"\"\"\n",
    "    \n",
    "    s3_client = boto3.client('s3', aws_access_key_id= ACCESS_KEY_ID,\n",
    "                    aws_secret_access_key=SECRET_ACCESS_KEY_ID)\n",
    "    \n",
    "    with open(RESULT_IMG_PATH, \"rb\") as f:\n",
    "        s3_client.upload_fileobj(f, bucket_name, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3edca0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predctions_forall_locations(BID,FID,model=MODEL):\n",
    "    \n",
    "    \"\"\" This function loads all the images form s3 bucket and call the ML model and get predcitions.\n",
    "    then saved those predictons in dict and creates folder treee at s3 bucket and then saved the outcome images at s3.\n",
    "    then create a mysql table and save the data into it.\n",
    "    finally returns the results dict.\n",
    "    this function should use first time of all the farm image uploaded, or user wants to get predcitons of all the images\n",
    "    again form the begining\"\"\"\n",
    "    \n",
    "    # get the connection with s3 bucket\n",
    "    s3 = boto3.resource('s3',\n",
    "                    aws_access_key_id=ACCESS_KEY_ID,\n",
    "                    aws_secret_access_key=SECRET_ACCESS_KEY_ID)\n",
    "\n",
    "    bucket = s3.Bucket(BUCKET_NAME)\n",
    "    \n",
    "     \n",
    "    # get area-location code dict\n",
    "    codes_dict = collect_area_location_codes(BID,FID)\n",
    "    # main result\n",
    "    results_dic = {\"area_code\":[], \"location_code\":[],\"image_name\":[], \"classes\":[], \"confidence\":[], \"active_frame_count\":[]}\n",
    "    # checks and create results folder tree to save the resulting images\n",
    "    folder = f\"images_{BID}/results_{FID}\"\n",
    "    tree_exist = is_folder_exist(folder,bucket)\n",
    "    if not tree_exist:\n",
    "        create_results_folder_tree(BID,FID,codes_dict)\n",
    "        \n",
    "    #print(f\"images of BID:{BID}, FID:{FID}\")\n",
    "    #count = 0\n",
    "\n",
    "    for area_code,location_list in codes_dict.items():\n",
    "        for location_code in location_list:\n",
    "            \n",
    "            # load image paths\n",
    "            objects = list(bucket.objects.all().filter(Prefix=f\"images_{BID}/data_{FID}/{area_code}/{location_code}/\"))\n",
    "            objects = objects[1:]\n",
    "            \n",
    "            # if images are there\n",
    "            if len(objects) >=1:\n",
    "                #print(f\"AREA:{area_code} and LOCATION:{location_code}\")\n",
    "                for folder in objects:\n",
    "                    results_dic[\"area_code\"].append(area_code)\n",
    "                    results_dic[\"location_code\"].append(location_code) \n",
    "\n",
    "                    # read the image data from S3 bucket directly into memory\n",
    "                    img_data = bucket.Object(folder.key).get().get('Body').read()\n",
    "                    # convert image data into PIL image object\n",
    "                    img = Image.open(BytesIO(img_data))\n",
    "                    # rotate correction\n",
    "                    img =  orientaion_correction(img) \n",
    "                    img.save(TOBE_PREDICT_IMAGE_PATH)\n",
    "                    \n",
    "                    # get predictions\n",
    "                    prediction = model.predict(TOBE_PREDICT_IMAGE_PATH, save=True, conf=CONFIDENCE_LEVEL, exist_ok=True)\n",
    "                    for results in prediction:\n",
    "                        boxes = results.boxes\n",
    "                    classes = [int(item.item()) for item in boxes.cls]\n",
    "                    confidences = [round(item.item(),2) for item in boxes.conf]\n",
    "                    \n",
    "                    results_dic[\"classes\"].append(classes)\n",
    "                    results_dic[\"confidence\"].append(confidences)\n",
    "                    results_dic[\"active_frame_count\"].append(sum(classes))\n",
    "                    image_name = folder.key.split('/')[-1]\n",
    "                    results_dic[\"image_name\"].append(image_name)\n",
    "                    # upload the output image to s3 \n",
    "                    save_path = f\"images_{BID}/results_{FID}/{area_code}/{location_code}/{image_name}\"\n",
    "                    upload_image(RESULT_IMG_PATH,save_path,BUCKET_NAME)\n",
    "     \n",
    "            else:\n",
    "                results_dic[\"area_code\"].append(area_code)\n",
    "                results_dic[\"location_code\"].append(location_code) \n",
    "                results_dic[\"image_name\"].append([])\n",
    "                results_dic[\"classes\"].append([])\n",
    "                results_dic[\"confidence\"].append([]) \n",
    "                ## USED RANDOM VALUE TO CLACULATE POLLINATION MAP. WHEN ACTUAL CASE FILL THIS, USING np.NaN \n",
    "                results_dic[\"active_frame_count\"].append(random.randint(0, 40))\n",
    "\n",
    "    # creates a mysql table and store the results\n",
    "    dataset = pd.DataFrame(results_dic)\n",
    "    table_name = f\"{MYSQL_RESULRS_TABLE_PREFIX}_{BID}_{FID}\"\n",
    "    create_mysql_table(dataset, table_name, credentials=MYSQL_CREDENTIALS)\n",
    "    # update hive details table to pollination map\n",
    "    hive_details_table_name = f\"{HIVE_DETAILS_TABLE_PREFIX}_{BID}_{FID}\"\n",
    "    update_active_frame_counts(table_name, hive_details_table_name)\n",
    "    # create analytics table\n",
    "    create_analytics_table(BID,FID)\n",
    "    \n",
    "    return results_dic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca26f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predtictions_specific_location(BID,FID,area_code,location_code,model=MODEL):\n",
    "    \"\"\"This function get load images, get predictions, save resulting images and returns \n",
    "    resulting dictionary of data for a given area_code and location_code only. (only for a single location).\n",
    "    This function should use when a user update the images form a specific hive location.\"\"\"\n",
    "    # get the connection with s3 bucket\n",
    "    s3 = boto3.resource('s3',\n",
    "                        aws_access_key_id=ACCESS_KEY_ID,\n",
    "                        aws_secret_access_key=SECRET_ACCESS_KEY_ID)\n",
    "\n",
    "    bucket = s3.Bucket(BUCKET_NAME)\n",
    "    # main result\n",
    "    results_dic = {\"area_code\":[], \"location_code\":[],\"image_name\":[], \"classes\":[], \"confidence\":[], \"active_frame_count\":[]}\n",
    "    # load image paths\n",
    "    objects = list(bucket.objects.all().filter(Prefix=f\"images_{BID}/data_{FID}/{area_code}/{location_code}/\"))\n",
    "    objects = objects[1:]\n",
    "    \n",
    "    # if images are there\n",
    "    if len(objects) >=1:\n",
    "\n",
    "        #print(f\"AREA:{area_code} and LOCATION:{location_code}\")\n",
    "        for folder in objects:\n",
    "            results_dic[\"area_code\"].append(area_code)\n",
    "            results_dic[\"location_code\"].append(location_code) \n",
    "\n",
    "            # read the image data from S3 bucket directly into memory\n",
    "            img_data = bucket.Object(folder.key).get().get('Body').read()\n",
    "            # convert image data into PIL image object\n",
    "            img = Image.open(BytesIO(img_data))\n",
    "            # rotate correction\n",
    "            img =  orientaion_correction(img) \n",
    "            img.save(TOBE_PREDICT_IMAGE_PATH)\n",
    "\n",
    "            # get predictions\n",
    "            prediction = model.predict(TOBE_PREDICT_IMAGE_PATH, save=True, conf=CONFIDENCE_LEVEL, exist_ok=True)\n",
    "            for results in prediction:\n",
    "                boxes = results.boxes\n",
    "            classes = [int(item.item()) for item in boxes.cls]\n",
    "            confidences = [round(item.item(),2) for item in boxes.conf]\n",
    "\n",
    "            results_dic[\"classes\"].append(classes)\n",
    "            results_dic[\"confidence\"].append(confidences)\n",
    "            results_dic[\"active_frame_count\"].append(sum(classes))\n",
    "            image_name = folder.key.split('/')[-1]\n",
    "            results_dic[\"image_name\"].append(image_name)\n",
    "\n",
    "            save_path = f\"images_{BID}/results_{FID}/{area_code}/{location_code}/{image_name}\"\n",
    "            upload_image(RESULT_IMG_PATH,save_path,BUCKET_NAME)\n",
    "    else:\n",
    "        results_dic[\"area_code\"].append(area_code)\n",
    "        results_dic[\"location_code\"].append(location_code) \n",
    "        results_dic[\"image_name\"].append([])\n",
    "        results_dic[\"confidence\"].append([]) \n",
    "        results_dic[\"classes\"].append([])\n",
    "\n",
    "        ## USED RANDOM VALUE TO CLACULATE POLLINATION MAP. WHEN ACTUAL CASE FILL THIS, USING np.NaN \n",
    "        results_dic[\"active_frame_count\"].append(random.randint(0, 40)) \n",
    "        \n",
    "    # update the ml results table by deleteing old data and inserting new data for the specified location\n",
    "    table_name = f\"{MYSQL_RESULRS_TABLE_PREFIX}_{BID}_{FID}\"    \n",
    "    delete_data(table_name,area_code,location_code,credentials=MYSQL_CREDENTIALS)\n",
    "    insert_multiple_raws(table_name, results_dic, credentials=MYSQL_CREDENTIALS)\n",
    "    # update hive details table to pollination map\n",
    "    hive_details_table_name = f\"{HIVE_DETAILS_TABLE_PREFIX}_{BID}_{FID}\"\n",
    "    update_active_frame_counts(table_name, hive_details_table_name)\n",
    "    # update analytics table\n",
    "    create_analytics_table(BID,FID)\n",
    "\n",
    "    return results_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c37cc9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = load_images_get_predctions(BID,FID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3161c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fe9dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f235e3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3 = boto3.resource(\\'s3\\',\\n                    aws_access_key_id= \\'AKIA4EQ6TDBWJ7BM5DK7\\',\\n                    aws_secret_access_key=\\'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\\')\\n\\nbucket = s3.Bucket(\\'beehive-thermal-images-testing\\')\\n\\n# specify the image and its key in the bucket\\nimage_key = f\"data-{BID}-{FID}/1/11139/FLIR0222.jpg\"\\n\\n# read the image data from S3 bucket directly into memory\\nimg_data = bucket.Object(image_key).get().get(\\'Body\\').read()\\n\\n# convert image data into PIL image object\\nimg = Image.open(BytesIO(img_data))\\n\\n# do something with the image object, e.g. display it\\n#img.show()\\nimg.save(\"new_img.png\")\\n\\nimg'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"s3 = boto3.resource('s3',\n",
    "                    aws_access_key_id= 'AKIA4EQ6TDBWJ7BM5DK7',\n",
    "                    aws_secret_access_key='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')\n",
    "\n",
    "bucket = s3.Bucket('beehive-thermal-images-testing')\n",
    "\n",
    "# specify the image and its key in the bucket\n",
    "image_key = f\"data-{BID}-{FID}/1/11139/FLIR0222.jpg\"\n",
    "\n",
    "# read the image data from S3 bucket directly into memory\n",
    "img_data = bucket.Object(image_key).get().get('Body').read()\n",
    "\n",
    "# convert image data into PIL image object\n",
    "img = Image.open(BytesIO(img_data))\n",
    "\n",
    "# do something with the image object, e.g. display it\n",
    "#img.show()\n",
    "img.save(\"new_img.png\")\n",
    "\n",
    "img\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
