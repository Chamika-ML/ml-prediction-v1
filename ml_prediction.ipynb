{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb744a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.228 ðŸš€ Python-3.11.5 torch-2.1.1 CPU (Intel Core(TM) i5-7300U 2.60GHz)\n",
      "Setup complete âœ… (4 CPUs, 7.9 GB RAM, 143.4/237.8 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    " \n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "from ultralytics import YOLO\n",
    "\n",
    "#!pip install mysql-connector-python\n",
    "import mysql.connector\n",
    "#!pip install sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#BID = \"B456\"\n",
    "#FID = \"123\"\n",
    "\n",
    "BID = \"\"\n",
    "FID = \"\"\n",
    "TOBE_PREDICT_IMAGE_PATH = f\"./images/need_to_predict_{BID}_{FID}.png\"\n",
    "RESULT_IMG_PATH =  f\"./runs/detect/predict/need_to_predict_{BID}_{FID}.png\"\n",
    "MODEL = YOLO(\"./model//best.pt\")\n",
    "CONFIDENCE_LEVEL = 0.5\n",
    "\n",
    "ACCESS_KEY_ID = \"AKIA4EQ6TDBWJ7BM5DK7\"\n",
    "SECRET_ACCESS_KEY_ID = \"9zO14I1rRtGmiSBKEc2X70Inc101SpDL7BsWrtqD\"\n",
    "BUCKET_NAME = \"beehive-thermal-images-testing\"\n",
    "\n",
    "MYSQL_CREDENTIALS = {\"host\":\"127.0.0.1\", \"user\":\"dilshan\", \"password\":\"1234\", \"database\":\"broodbox_results\", \"port\":3306}\n",
    "MYSQL_RESULRS_TABLE_PREFIX = \"ml_results\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5695e9d8",
   "metadata": {},
   "source": [
    "## Database functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "698bbadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mysql_table(dataset, table_name, credentials=MYSQL_CREDENTIALS):\n",
    "    \n",
    "    \"this function creates a table in mysql database using pandas dataframe\"\n",
    "    \n",
    "    engine = create_engine(f'mysql+mysqlconnector://{credentials[\"user\"]}:{credentials[\"password\"]}@{credentials[\"host\"]}:{credentials[\"port\"]}/{credentials[\"database\"]}', connect_args={\"connect_timeout\": 28800})\n",
    "    # Serialize lists into JSON strings\n",
    "    dataset[\"classes\"] = dataset[\"classes\"].apply(json.dumps)\n",
    "    dataset[\"confidence\"] = dataset[\"confidence\"].apply(json.dumps)\n",
    "    \n",
    "    dataset.to_sql(table_name, con=engine, if_exists='replace', index=False)\n",
    "    engine.dispose()\n",
    "\n",
    "\n",
    "    \n",
    "def delete_data(table_name,area_code,location_code,credentials=MYSQL_CREDENTIALS):  \n",
    "    \"\"\"This function will delete the raws where the area_code and location_code matches.\"\"\"\n",
    "    \n",
    "    # Connect to the MySQL server\n",
    "    connection = mysql.connector.connect(\n",
    "        host=credentials[\"host\"],\n",
    "        user=credentials[\"user\"],\n",
    "        password=credentials[\"password\"],\n",
    "        database=credentials[\"database\"]\n",
    "    )\n",
    "    \n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    insert_sql = f\"\"\"\n",
    "    DELETE FROM {table_name} WHERE area_code='{area_code}' AND location_code='{location_code}'\n",
    "    \"\"\"\n",
    "    cursor.execute(insert_sql)\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "\n",
    "def insert_multiple_raws(table_name, data, credentials=MYSQL_CREDENTIALS):\n",
    "    \"\"\" This function will inset multiple raws of data points to the given table.\n",
    "    the insert data shoulb be a dictionary\"\"\"\n",
    "    \n",
    "    # Connect to the MySQL server\n",
    "    connection = mysql.connector.connect(\n",
    "        host=credentials[\"host\"],\n",
    "        user=credentials[\"user\"],\n",
    "        password=credentials[\"password\"],\n",
    "        database=credentials[\"database\"]\n",
    "    )\n",
    "    \n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    # Prepare the INSERT query\n",
    "    insert_query = f\"\"\"\n",
    "    INSERT INTO {table_name} (area_code, location_code, classes, confidence, total_active_frames)\n",
    "    VALUES (%s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    # dictionary as a list of tuples of data points\n",
    "    insert_values =[(area_code, location_code, json.dumps(classes), json.dumps(confidence), total_active_frames) \n",
    "                    for area_code, location_code, classes, confidence, total_active_frames \n",
    "                    in list(zip(data['area_code'], data['location_code'], data['classes'], data['confidence'], data['total_active_frames']))]\n",
    "\n",
    "    # Execute the INSERT query with executemany\n",
    "    cursor.executemany(insert_query, insert_values) \n",
    "    # Commit the transaction\n",
    "    connection.commit()  \n",
    "    cursor.close()\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9a8cb9",
   "metadata": {},
   "source": [
    "## ML prediction functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb7aabc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_area_location_codes(BID,FID):\n",
    "    \n",
    "    \"\"\" This function returns a dict that contained each area code as keys and location codes of each area code as values.\n",
    "     the output format is {\"area_code1\":[list of location codes of that area1],.....,} \"\"\"\n",
    "    \n",
    "    url = f\"http://ec2-54-206-119-102.ap-southeast-2.compute.amazonaws.com:5000/hive/area-location-codes/{BID}/{FID}\"\n",
    "    response = requests.get(url)\n",
    "    # this contaied all area and loation codes separately as lists of a given farm\n",
    "    data = response.json()\n",
    "    area_codes = data[\"area_codes\"]\n",
    "\n",
    "    url_all = f\"http://ec2-54-206-119-102.ap-southeast-2.compute.amazonaws.com:5000/hive/{BID}/{FID}\"\n",
    "    response_all = requests.get(url_all)\n",
    "    # this contained all the hive details of given farm\n",
    "    data_all = response_all.json()\n",
    "\n",
    "    # this dict is the requried output format. it should contained as {\"area_code\":[list of location codes of that area]}\n",
    "    codes_dict = dict()\n",
    "    for area_code in area_codes:\n",
    "        codes_dict[area_code] = []\n",
    "\n",
    "        for location in data_all[\"hive_details\"]:\n",
    "\n",
    "            if location[\"area_code\"]==area_code:\n",
    "                codes_dict[area_code].append(location[\"location_code\"])\n",
    "                \n",
    "    return codes_dict \n",
    "\n",
    "\n",
    "def orientaion_correction(img):\n",
    "    \n",
    "    \"\"\"This function checks the given image rotated (480*640 ---> 640*480) or not. \n",
    "    if rotated then it rotate again to oraginal format (480*640 or 640*480 ) and returns the image.\n",
    "    if not rotated then it returns the original image.\"\"\"\n",
    "    \n",
    "    # getexif attribute is a method used to retrieve Exif (Exchangeable image file format) metadata from the image.\n",
    "    if hasattr(img, '_getexif') and img._getexif():\n",
    "        exif_data = img._getexif()\n",
    "        # 274 represents the Exif tag for orientation\n",
    "        orientation = exif_data.get(274) \n",
    "\n",
    "        # If orientation is 6, rotate clockwise by 90 degrees\n",
    "        if orientation == 6:  \n",
    "            img = img.transpose(method=Image.ROTATE_270)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def create_results_folder_tree(BID,FID,codes_dict):\n",
    "    \n",
    "    \"\"\" This function will creates object detection results saving folder structure like data folder\"\"\"\n",
    "    \n",
    "    # get connection with s3 and create resluts folder\n",
    "    s3_client = boto3.client('s3', aws_access_key_id= ACCESS_KEY_ID,\n",
    "                    aws_secret_access_key=SECRET_ACCESS_KEY_ID)\n",
    "    s3_client.put_object(Bucket=BUCKET_NAME, Key=f\"results_{BID}_{FID}/\")\n",
    "\n",
    "    for area_code,location_list in codes_dict.items():\n",
    "        s3_client.put_object(Bucket=BUCKET_NAME, Key=f\"results_{BID}_{FID}/{area_code}/\")\n",
    "\n",
    "        for location_code in location_list:\n",
    "            s3_client.put_object(Bucket=BUCKET_NAME, Key=f\"results_{BID}_{FID}/{area_code}/{location_code}/\")\n",
    "            \n",
    "def is_folder_exist(folder,bucket):\n",
    "    \n",
    "    \"\"\" If given folder exist bucket then this will returns true. other wise false.\"\"\"\n",
    "    \n",
    "    bucket_list = list(bucket.objects.all().filter(Prefix=f\"{folder}/\"))\n",
    "    if len(bucket_list) >=1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def upload_image(RESULT_IMG_PATH,save_path,bucket_name):\n",
    "    \n",
    "    \"\"\" This function save image at given save_path at s3 bucket\"\"\"\n",
    "    \n",
    "    s3_client = boto3.client('s3', aws_access_key_id= ACCESS_KEY_ID,\n",
    "                    aws_secret_access_key=SECRET_ACCESS_KEY_ID)\n",
    "    \n",
    "    with open(RESULT_IMG_PATH, \"rb\") as f:\n",
    "        s3_client.upload_fileobj(f, bucket_name, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3edca0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predctions_forall_locations(BID,FID,model=MODEL):\n",
    "    \n",
    "    \"\"\" This function loads all the images form s3 bucket and call the ML model and get predcitions.\n",
    "    then saved those predictons in dict and creates folder treee at s3 bucket and then saved the outcome images at s3.\n",
    "    then create a mysql table and save the data into it.\n",
    "    finally returns the results dict.\n",
    "    this function should use first time of all the farm image uploaded, or user wants to get predcitons of all the images\n",
    "    again form the begining\"\"\"\n",
    "    \n",
    "    # get the connection with s3 bucket\n",
    "    s3 = boto3.resource('s3',\n",
    "                    aws_access_key_id=ACCESS_KEY_ID,\n",
    "                    aws_secret_access_key=SECRET_ACCESS_KEY_ID)\n",
    "\n",
    "    bucket = s3.Bucket(BUCKET_NAME)\n",
    "    \n",
    "     \n",
    "    # get area-location code dict\n",
    "    codes_dict = collect_area_location_codes(BID,FID)\n",
    "    # main result\n",
    "    results_dic = {\"area_code\":[], \"location_code\":[], \"classes\":[], \"confidence\":[], \"total_active_frames\":[]}\n",
    "    # checks and create results folder tree to save the resulting images\n",
    "    folder = f\"results_{BID}_{FID}\"\n",
    "    tree_exist = is_folder_exist(folder,bucket)\n",
    "    if not tree_exist:\n",
    "        create_results_folder_tree(BID,FID,codes_dict)\n",
    "        \n",
    "    #print(f\"images of BID:{BID}, FID:{FID}\")\n",
    "    #count = 0\n",
    "\n",
    "    for area_code,location_list in codes_dict.items():\n",
    "        for location_code in location_list:\n",
    "            \n",
    "            # load image paths\n",
    "            objects = list(bucket.objects.all().filter(Prefix=f\"data-{BID}-{FID}/{area_code}/{location_code}/\"))\n",
    "            objects = objects[1:]\n",
    "            \n",
    "            # if images are there\n",
    "            if len(objects) >=1:\n",
    "                #print(f\"AREA:{area_code} and LOCATION:{location_code}\")\n",
    "                for folder in objects:\n",
    "                    results_dic[\"area_code\"].append(area_code)\n",
    "                    results_dic[\"location_code\"].append(location_code) \n",
    "\n",
    "                    # read the image data from S3 bucket directly into memory\n",
    "                    img_data = bucket.Object(folder.key).get().get('Body').read()\n",
    "                    # convert image data into PIL image object\n",
    "                    img = Image.open(BytesIO(img_data))\n",
    "                    # rotate correction\n",
    "                    img =  orientaion_correction(img) \n",
    "                    img.save(TOBE_PREDICT_IMAGE_PATH)\n",
    "                    \n",
    "                    # get predictions\n",
    "                    prediction = model.predict(TOBE_PREDICT_IMAGE_PATH, save=True, conf=CONFIDENCE_LEVEL, exist_ok=True)\n",
    "                    for results in prediction:\n",
    "                        boxes = results.boxes\n",
    "                    classes = [int(item.item()) for item in boxes.cls]\n",
    "                    confidences = [round(item.item(),2) for item in boxes.conf]\n",
    "                    \n",
    "                    results_dic[\"classes\"].append(classes)\n",
    "                    results_dic[\"confidence\"].append(confidences)\n",
    "                    results_dic[\"total_active_frames\"].append(sum(classes))\n",
    "                    \n",
    "                    # upload the output image to s3 \n",
    "                    save_path = f\"results_{BID}_{FID}/{area_code}/{location_code}/{folder.key.split('/')[-1]}\"\n",
    "                    upload_image(RESULT_IMG_PATH,save_path,BUCKET_NAME)\n",
    "     \n",
    "            else:\n",
    "                results_dic[\"area_code\"].append(area_code)\n",
    "                results_dic[\"location_code\"].append(location_code) \n",
    "                results_dic[\"confidence\"].append([]) \n",
    "                results_dic[\"classes\"].append([])\n",
    "\n",
    "                ## USED RANDOM VALUE TO CLACULATE POLLINATION MAP. WHEN ACTUAL CASE FILL THIS, USING np.NaN \n",
    "                results_dic[\"total_active_frames\"].append(random.randint(0, 40))\n",
    "\n",
    "    # creates a mysql table and store the results\n",
    "    dataset = pd.DataFrame(results_dic)\n",
    "    table_name = f\"{MYSQL_RESULRS_TABLE_PREFIX}_{BID}_{FID}\"\n",
    "    create_mysql_table(dataset, table_name, credentials=MYSQL_CREDENTIALS)\n",
    "    \n",
    "    return results_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ca26f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predtictions_specific_location(BID,FID,area_code,location_code,model=MODEL):\n",
    "    \"\"\"This function get load images, get predictions, save resulting images and returns \n",
    "    resulting dictionary of data for a given area_code and location_code only. (only for a single location).\n",
    "    This function should use when a user update the images form a specific hive location.\"\"\"\n",
    "    # get the connection with s3 bucket\n",
    "    s3 = boto3.resource('s3',\n",
    "                        aws_access_key_id=ACCESS_KEY_ID,\n",
    "                        aws_secret_access_key=SECRET_ACCESS_KEY_ID)\n",
    "\n",
    "    bucket = s3.Bucket(BUCKET_NAME)\n",
    "    # main result\n",
    "    results_dic = {\"area_code\":[], \"location_code\":[], \"classes\":[], \"confidence\":[], \"total_active_frames\":[]}\n",
    "    # load image paths\n",
    "    objects = list(bucket.objects.all().filter(Prefix=f\"data-{BID}-{FID}/{area_code}/{location_code}/\"))\n",
    "    objects = objects[1:]\n",
    "    \n",
    "    # if images are there\n",
    "    if len(objects) >=1:\n",
    "\n",
    "        #print(f\"AREA:{area_code} and LOCATION:{location_code}\")\n",
    "        for folder in objects:\n",
    "            results_dic[\"area_code\"].append(area_code)\n",
    "            results_dic[\"location_code\"].append(location_code) \n",
    "\n",
    "            # read the image data from S3 bucket directly into memory\n",
    "            img_data = bucket.Object(folder.key).get().get('Body').read()\n",
    "            # convert image data into PIL image object\n",
    "            img = Image.open(BytesIO(img_data))\n",
    "            # rotate correction\n",
    "            img =  orientaion_correction(img) \n",
    "            img.save(TOBE_PREDICT_IMAGE_PATH)\n",
    "\n",
    "            # get predictions\n",
    "            prediction = model.predict(TOBE_PREDICT_IMAGE_PATH, save=True, conf=CONFIDENCE_LEVEL, exist_ok=True)\n",
    "            for results in prediction:\n",
    "                boxes = results.boxes\n",
    "            classes = [int(item.item()) for item in boxes.cls]\n",
    "            confidences = [round(item.item(),2) for item in boxes.conf]\n",
    "\n",
    "            results_dic[\"classes\"].append(classes)\n",
    "            results_dic[\"confidence\"].append(confidences)\n",
    "            results_dic[\"total_active_frames\"].append(sum(classes))\n",
    "\n",
    "            save_path = f\"results_{BID}_{FID}/{area_code}/{location_code}/{folder.key.split('/')[-1]}\"\n",
    "            upload_image(RESULT_IMG_PATH,save_path,BUCKET_NAME)\n",
    "    else:\n",
    "        results_dic[\"area_code\"].append(area_code)\n",
    "        results_dic[\"location_code\"].append(location_code) \n",
    "        results_dic[\"confidence\"].append([]) \n",
    "        results_dic[\"classes\"].append([])\n",
    "\n",
    "        ## USED RANDOM VALUE TO CLACULATE POLLINATION MAP. WHEN ACTUAL CASE FILL THIS, USING np.NaN \n",
    "        results_dic[\"total_active_frames\"].append(random.randint(0, 40)) \n",
    "        \n",
    "    # update the ml results table by deleteing old data and inserting new data for the specified location\n",
    "    table_name = f\"{MYSQL_RESULRS_TABLE_PREFIX}_{BID}_{FID}\"    \n",
    "    delete_data(table_name,area_code,location_code,credentials=MYSQL_CREDENTIALS)\n",
    "    insert_multiple_raws(table_name, results_dic, credentials=MYSQL_CREDENTIALS)\n",
    "   \n",
    "    return results_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c37cc9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = load_images_get_predctions(BID,FID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3161c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fe9dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f235e3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3 = boto3.resource(\\'s3\\',\\n                    aws_access_key_id= \\'AKIA4EQ6TDBWJ7BM5DK7\\',\\n                    aws_secret_access_key=\\'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\\')\\n\\nbucket = s3.Bucket(\\'beehive-thermal-images-testing\\')\\n\\n# specify the image and its key in the bucket\\nimage_key = f\"data-{BID}-{FID}/1/11139/FLIR0222.jpg\"\\n\\n# read the image data from S3 bucket directly into memory\\nimg_data = bucket.Object(image_key).get().get(\\'Body\\').read()\\n\\n# convert image data into PIL image object\\nimg = Image.open(BytesIO(img_data))\\n\\n# do something with the image object, e.g. display it\\n#img.show()\\nimg.save(\"new_img.png\")\\n\\nimg'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"s3 = boto3.resource('s3',\n",
    "                    aws_access_key_id= 'AKIA4EQ6TDBWJ7BM5DK7',\n",
    "                    aws_secret_access_key='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')\n",
    "\n",
    "bucket = s3.Bucket('beehive-thermal-images-testing')\n",
    "\n",
    "# specify the image and its key in the bucket\n",
    "image_key = f\"data-{BID}-{FID}/1/11139/FLIR0222.jpg\"\n",
    "\n",
    "# read the image data from S3 bucket directly into memory\n",
    "img_data = bucket.Object(image_key).get().get('Body').read()\n",
    "\n",
    "# convert image data into PIL image object\n",
    "img = Image.open(BytesIO(img_data))\n",
    "\n",
    "# do something with the image object, e.g. display it\n",
    "#img.show()\n",
    "img.save(\"new_img.png\")\n",
    "\n",
    "img\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
